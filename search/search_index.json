{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>I'm a Deep Learning and Machine Learning Researcher, currently working on various topics in Deep Learning.</p> <p>I currently work on Automatic Speech Recognition via CTC/Transducer models. I enjoy working on Image Classification  and many of its sub-domains and Time Series Classification.</p>"},{"location":"academia/","title":"Education","text":""},{"location":"academia/#master-of-science-in-computer-science","title":"Master of Science in Computer Science","text":"<p>University of Illinois at Chicago : 3.77 / 4.0 2017-2018, Chicago, IL, USA</p>"},{"location":"academia/#bachelor-of-engineering-in-computer-science","title":"Bachelor of Engineering in Computer Science","text":"<p>D.J. Sanghvi College of Engineering : 8.0 / 10.0 2012-2016, Mumbai, Maharashtra, India</p>"},{"location":"contact/","title":"Contact","text":"<p>Contact information:</p> <ul> <li>Resume : PDF</li> <li>Github : titu1994</li> <li>LinkedIn : https://www.linkedin.com/in/somshubramajumdar/</li> <li>Twitter : @haseoX94</li> <li>Mail : titu1994@gmail.com</li> </ul>"},{"location":"projects/android/","title":"Android Projects","text":""},{"location":"projects/android/#ragial-notifier","title":"Ragial Notifier","text":"<p>Android application that connects to Ragial, the online browser to International Ragnarok Online's market place, and notifies the user of when marked items are being sold at a sale.</p>"},{"location":"projects/android/#ragial-searcher","title":"Ragial Searcher","text":"<p>The java core library built to query and parse Ragial for details about products.</p>"},{"location":"projects/dl/additional/","title":"Various Projects","text":""},{"location":"projects/dl/additional/#keras-lamb-optimizer","title":"Keras LAMB Optimizer","text":"<p>Implementation of the LAMB optimizer from the paper Reducing BERT Pre-Training Time from 3 Days to 76 Minutes.</p> <p>Supports huge batch size training while only requiring learning rate to be changed.</p>"},{"location":"projects/dl/additional/#keras-adabound-optimizer","title":"Keras AdaBound Optimizer","text":"<p>Keras port of AdaBound Optimizer for PyTorch, from the paper Adaptive Gradient Methods with Dynamic Bound of Learning Rate.</p> <p>Provides weights for ResNet 34 model trained on CIFAR 10 reaching 92% without random cropping augmentation.</p>"},{"location":"projects/dl/additional/#dynamic-time-warping-numba","title":"Dynamic Time Warping - Numba","text":"<p>Implementation of Dynamic Time Warping algorithm with speed improvements based on Numba.</p> <p>Supports for K nearest neighbours classifier using Dynamic Time Warping, based on the work presented by Mark Regan. The classes called KnnDTW are obtained from there, as a simplified interface akin to Scikit-Learn.</p> <p>The three variants available are in dtw.py, odtw.py and ucrdtw.py.</p> <ul> <li>dtw.py: Single threaded variant, support for visualizing the progress bar.</li> <li>odtw.py: Multi threaded variant, no support for visualization. In practice, much more effiecient.</li> <li>ucrdtw.py: Experimental (Do not use). Multi threaded variant, no support for visualization. It is based upon the optimized C implementation available at https://github.com/klon/ucrdtw.</li> </ul>"},{"location":"projects/dl/additional/#neural-algorithmic-logic-units","title":"Neural Algorithmic Logic Units","text":"<p>A Keras implementation of Neural Arithmatic and Logical Unit from the paper Neural Algorithmic Logic Units by Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, Phil Blunsom.</p> <ul> <li>Contains the layers for <code>Neural Arithmatic Logic Unit (NALU)</code> and <code>Neural Accumulator (NAC)</code>.</li> <li>Also contains the results of the static function learning toy tests.</li> </ul>"},{"location":"projects/dl/additional/#padam-partially-adaptive-momentum-estimation","title":"Padam - Partially Adaptive Momentum Estimation","text":"<p>Keras implementation of Padam from Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks.</p> <p>Padam allows for much larger learning rates to be utilized, and follows generalization closely with Stochastc Gradient Descent.</p>"},{"location":"projects/dl/additional/#neural-image-assessment","title":"Neural Image Assessment","text":"<p>Implementation of NIMA: Neural Image Assessment in Keras + Tensorflow with weights for MobileNet model trained on AVA dataset.</p> <p>NIMA assigns a Mean + Standard Deviation score to images, and can be used as a tool to automatically inspect quality of images or as a loss function to further improve the quality of generated images.</p> <p>Contains weights trained on the AVA dataset for the following models:</p> <ul> <li>NASNet Mobile (0.067 EMD on valset thanks to @tfriedel !, 0.0848 EMD with just pre-training)</li> <li>Inception ResNet v2 (~ 0.07 EMD on valset, thanks to @tfriedel !)</li> <li>MobileNet (0.0804 EMD on valset)</li> </ul>"},{"location":"projects/dl/additional/#switch-normalization","title":"Switch Normalization","text":"<p>Switchable Normalization is a normalization technique that is able to learn different normalization operations for different normalization layers in a deep neural network in an end-to-end manner.</p> <p>Keras port of the implementation of the paper Differentiable Learning-to-Normalize via Switchable Normalization.</p> <p>Code ported from the switchnorm official repository.</p>"},{"location":"projects/dl/additional/#group-normalization","title":"Group Normalization","text":"<p>A Keras implementation of Group Normalization by Yuxin Wu and Kaiming He.</p> <p>Useful for fine-tuning of large models on smaller batch sizes than in research setting (where batch size is very large due to multiple GPUs). Similar to Batch Renormalization, but performs significantly better on ImageNet.</p> <p>Available in Keras Contrib inside normalization module.</p>"},{"location":"projects/dl/additional/#batch-renormalization","title":"Batch Renormalization","text":"<p>Batch Renormalization algorithm implementation in Keras 2.0+. Original paper by Sergey Ioffe, Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models.</p> <p>Available in Keras Contrib inside normalization module.</p>"},{"location":"projects/dl/additional/#snapshot-ensembles","title":"Snapshot Ensembles","text":"<p>Implementation of the paper Snapshot Ensembles: Train 1, Get M for Free in Keras 2+</p>"},{"location":"projects/dl/additional/#tiramisu-densenets-for-semantic-segmentation","title":"Tiramisu DenseNets for Semantic Segmentation","text":"<p>Fully Connected DenseNet for Image Segmentation implementation of the paper The One Hundred Layers Tiramisu : Fully Convolutional DenseNets for Semantic Segmentation</p> <p>Available in Keras Contrib inside the DenseNet applications module.</p>"},{"location":"projects/dl/additional/#one-cycle-learning-policy","title":"One Cycle Learning Policy","text":"<p>Implementation of One-Cycle Learning rate policy from the papers by Leslie N. Smith.</p> <ul> <li>A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay</li> <li>Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates</li> </ul> <p>Contains two Keras callbacks, LRFinder and OneCycleLR which are ported from the PyTorch Fast.ai library.</p>"},{"location":"projects/dl/additional/#normalized-optimizers","title":"Normalized Optimizers","text":"<p>Keras wrapper class for Normalized Gradient Descent from kmkolasinski/max-normed-optimizer, which can be applied to almost all Keras optimizers. Partially implements Block-Normalized Gradient Method: An Empirical Study for Training Deep Neural Network for all base Keras optimizers, and allows flexibility to choose any normalizing function. It does not implement adaptive learning rates however.</p> <p>The wrapper class can also be extended to allow Gradient Masking and Gradient Clipping using custom norm metrics.</p> <p>Wrapper classes :</p> <ul> <li>NormalizedOptimizer: To normalize of gradient by the norm of that gradient.</li> <li>ClippedOptimizer: To clip the gradient by the norm of that gradient. Note: Clips by Local Norm only !</li> </ul>"},{"location":"projects/dl/additional/#mobile-colorizer","title":"Mobile Colorizer","text":"<p>Utilizes a U-Net inspired model conditioned on MobileNet class features to generate a mapping from Grayscale to Color image. Based on the work https://github.com/baldassarreFe/deep-koalarization</p> <p>Uses MobileNets for memory efficiency in comparison to Inception-ResNet-V2 so that training can be done on a single GPU (of 4 GB size minimum).</p>"},{"location":"projects/dl/additional/#tensorflow-eager-execution-examples","title":"Tensorflow Eager Execution Examples","text":"<p>Tensorflow Eager Execution mode allows an imperative programming style, similar to Numpy in addition to nearly all of the Tensorflow graph APIs, higher level APIs to build models (Keras) as well as easy debugging with the Python debug bridge.</p> <p>Since Eager Execution APIs are quite recent, some kinks still exist, but as of this moment, they are minor and can be sidesteped. These issues are highlighted in the notebooks and it is advised to browse through the comments, even if the topic is easy, so as to understand the limitations of Eager as TF 1.8.</p> <p>The following set of examples show usage of higher level APIs of Keras, different ways of performing the same thing, some issues that can arise and how to sidestep them while we wait for updates in Tensorflow to fix them.</p> <p>It is to be noted, that I try to replicate most parts of this excellent PyTorch Tutorial Set. A few topics are missing - such as GANs and Image Captioning since I do not have the computational resources to train such models. A notable exception is Style Transfer, for which I have another repository dedicated to it, so I won't be porting it to Eager.</p> <p>A final note :</p> <ul> <li>Eager is evolving rapidly, and almost all of these issues that I stated here are edge cases that can/will be resolved in a later update. I still appreciate Eager, even with its limitations, as it offers a rich set of APIs from its Tensorflow heritage in an imperative execution environment like PyTorch.</li> <li>This means that once the Eager API has all of its kinks ironed out, it will result in cleaner, more concise code and hopefully at performance close to Tensorflow itself.</li> </ul>"},{"location":"projects/dl/additional/#twitter-sentiment-analysis","title":"Twitter Sentiment Analysis","text":"<p>To perform sentiment analysis over a corpus of tweets during the U.S. 2012 Re-Election about the candidates Barack Obama and Mitt Romney.</p> <p>The previous best score on the test dataset was 64 % f1-score, suggesting that improvements can be obtained using modern machine learning / deep learning algorithms.</p>"},{"location":"projects/dl/automatic-speech-recognition/","title":"Automatic Speech Recognition","text":""},{"location":"projects/dl/automatic-speech-recognition/#numba-cuda-warp-transducer-loss","title":"Numba CUDA Warp Transducer Loss","text":"<p>Warp RNN Transducer Loss for ASR in Pytorch, ported from  HawkAaron/warp-transducer and a replica of the stable version in NVIDIA Neural Module repository (NVIDIA NeMo).</p>"},{"location":"projects/dl/contrib/","title":"Contributions to Keras and Keras Contrib","text":""},{"location":"projects/dl/contrib/#keras-applications","title":"Keras Applications","text":"<p>Various state of the art models listed below were merged with Keras or Keras contrib as below :</p> <ul> <li>NASNet <code>(Keras)</code></li> <li>MobilNet V1 <code>(Keras)</code></li> <li>NASNet <code>(Keras-Contrib)</code></li> <li>DenseNet <code>(Keras-Contrib)</code></li> <li>Wide ResNet <code>(Keras-Contrib)</code></li> <li>Residual of Residual Networks <code>(Keras-Contrib)</code></li> </ul>"},{"location":"projects/dl/contrib/#contrib-callbacks","title":"Contrib Callbacks","text":"<p>Added the Snapshot Ensemble callback manager for Contrib.</p>"},{"location":"projects/dl/contrib/#contrib-layers","title":"Contrib Layers","text":"<p>Added a few layers to Keras Contrib :</p> <ul> <li><code>SubPixelUpscaling</code></li> <li><code>BatchRenormalization</code></li> <li><code>GroupNormalization</code></li> </ul>"},{"location":"projects/dl/image-classifiers/","title":"Keras Image Classifiers","text":"<p>Model building scripts which replicate the architectures of various state of the art papers. All of these models are built in Keras or Tensorflow.</p>"},{"location":"projects/dl/image-classifiers/#octave-convolutions","title":"Octave Convolutions","text":"<p>Keras implementation of the Octave Convolution blocks from the paper Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution.</p> <p>Provides code blocks for initializing the Oct-Conv architecture, building Oct-Conv blocks and closing the Oct-conv architecture, as well as Octave-Resnet models.</p>"},{"location":"projects/dl/image-classifiers/#non-local-neural-networks","title":"Non-Local Neural Networks","text":"<p>Keras implementation of Non-local blocks from the paper Non-local Neural Networks.</p> <ul> <li>Support for \"Gaussian\", \"Embedded Gaussian\" and \"Dot\" instantiations of the Non-Local block.</li> <li>Support for variable shielded computation mode (reduces computation by N**2 x, where N is default to 2)</li> <li>Support for \"Concatenation\" instantiation will be supported when authors release their code.</li> </ul>"},{"location":"projects/dl/image-classifiers/#squeeze-excitation-networks","title":"Squeeze &amp; Excitation Networks","text":"<p>Implementation of Squeeze and Excitation Networks, as an independent block that can be added to any Keras layer, or pre-built models such as :</p> <ul> <li>SE-ResNet. Custom ResNets can be built using the SEResNet model builder, whereas prebuilt Resnet models such as SEResNet50, SEResNet101 and SEResNet154 can also be built directly.</li> <li>SE-InceptionV3</li> <li>SE-Inception-ResNet-v2</li> <li>SE-ResNeXt</li> </ul> <p>Additional models (not from the paper, not verified if they improve performance)</p> <ul> <li>SE-MobileNets</li> <li>SE-DenseNet - Custom SE-DenseNets can be built using SEDenseNet model builder, whereas prebuilt SEDenseNet models such as SEDenseNetImageNet121, SEDenseNetImageNet169, SEDenseNetImageNet161, SEDenseNetImageNet201 and SEDenseNetImageNet264 can be build DenseNet in ImageNet configuration. To use SEDenseNet in CIFAR mode, use the SEDenseNet model builder.</li> </ul>"},{"location":"projects/dl/image-classifiers/#nasnet","title":"NASNet","text":"<p>An implementation of \"NASNet\" models from the paper Learning Transferable Architectures for Scalable Image Recognitio in Keras 2.0+.</p> <p>Based on the models described in the TFSlim implementation and some modules from the TensorNets implementation.</p> <p>Weights have been ported over from the official NASNet Tensorflow repository.</p>"},{"location":"projects/dl/image-classifiers/#mobilenets-v1-and-v2","title":"MobileNets V1 and V2","text":"<p>Keras implementation of the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.</p> <p>Contains the Keras implementation of the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks.</p> <p>Weights for all variants of MobileNet V1 and MobileNet V2 are available.</p>"},{"location":"projects/dl/image-classifiers/#sparsenets","title":"SparseNets","text":"<p>Keras Implementation of Sparse Networks from the paper Sparsely Connected Convolutional Networks.</p> <p>Code derived from the offical repository - https://github.com/Lyken17/SparseNet</p> <p>No weights available as they are not released.</p>"},{"location":"projects/dl/image-classifiers/#dual-path-networks","title":"Dual Path Networks","text":"<p>Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.</p> <p>Due to Keras and Tensorflow not supporting <code>Grouped Convolutions</code> yet, this is an inefficient implementation with no weights.</p>"},{"location":"projects/dl/image-classifiers/#resnext","title":"ResNeXt","text":"<p>Implementation of ResNeXt models from the paper Aggregated Residual Transformations for Deep Neural Networks in Keras 2.0+.</p> <p>Contains code for building the general ResNeXt model (optimized for datasets similar to CIFAR) and ResNeXtImageNet (optimized for the ImageNet dataset).</p> <p>Due to Keras and Tensorflow not supporting <code>Grouped Convolutions</code> yet, this is an inefficient implementation with no weights.</p>"},{"location":"projects/dl/image-classifiers/#inception-v4-inception-resnet-v2","title":"Inception v4 / Inception ResNet v2","text":"<p>Implementations of the Inception-v4, Inception - Resnet-v1 and v2 Architectures in Keras using the Functional API. The paper on these architectures is available at Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.</p> <p>Weights are provided for Inception v4 and Inception ResNet v2 Models.</p>"},{"location":"projects/dl/image-classifiers/#densenets","title":"DenseNets","text":"<p>DenseNet implementation of the paper Densely Connected Convolutional Networks in Keras</p> <p>Now supports the more efficient DenseNet-BC (DenseNet-Bottleneck-Compressed) networks. Using the DenseNet-BC-190-40 model, it obtaines state of the art performance on CIFAR-10 and CIFAR-100</p> <p>Weights are provided for DenseNet Models.</p>"},{"location":"projects/dl/image-classifiers/#wide-residual-networks","title":"Wide Residual Networks","text":"<p>Implementation of Wide Residual Networks from the paper Wide Residual Networks in Keras.</p> <p>No weights available due to limited computation available.</p>"},{"location":"projects/dl/image-classifiers/#residual-of-residual-networks","title":"Residual-of-Residual Networks","text":"<p>This is an implementation of the paper Residual Networks of Residual Networks: Multilevel Residual Networks</p>"},{"location":"projects/dl/nas/","title":"Neural Architecture Search","text":"<p>Utilization of a Controller using Reinforcement Learning, Sequentual Model Based Optimization to train via surrogate losses, or using XGBoost Trees to reduce the search space.</p>"},{"location":"projects/dl/nas/#neural-architecture-search_1","title":"Neural Architecture Search","text":"<p>Basic and limited (1 GPU) implementation of Controller RNN from Neural Architecture Search with Reinforcement Learning and Learning Transferable Architectures for Scalable Image Recognition.</p> <ul> <li>Uses Keras to define and train children / generated networks, which are defined in Tensorflow by the Controller RNN.</li> <li>Define a state space by using StateSpace, a manager which adds states and handles communication between the Controller RNN and the user.</li> <li>Controller manages the training and evaluation of the Controller RNN</li> <li>NetworkManager handles the training and reward computation of a Keras model</li> </ul>"},{"location":"projects/dl/nas/#progressive-neural-architecture-search","title":"Progressive Neural Architecture Search","text":"<p>Basic and limited (1 GPU) implementation of Encoder RNN from Progressive Neural Architecture Search.</p> <ul> <li>Uses Keras to define and train children / generated networks, which are found via sequential model-based optimization in Tensorflow, ranked by the Encoder RNN.</li> <li>Define a state space by using StateSpace, a manager which maintains input states and handles communication between the Encoder RNN and the user.</li> <li>Encoder manages the training and evaluation of the Encoder RNN</li> <li>NetworkManager handles the training and reward computation of the children Keras model</li> </ul>"},{"location":"projects/dl/nas/#sequentual-halving-and-classification","title":"Sequentual Halving and Classification","text":"<p><code>PySHAC</code> is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease.</p> <p>Note : This library is not affiliated with Google.</p> <p>Stable build documentation can be found at PySHAC Documentation.</p> <p>It contains a User Guide, as well as explanation of the different engines that can be used with PySHAC.</p>"},{"location":"projects/dl/style-transfer/","title":"Naural Style Transfer","text":""},{"location":"projects/dl/style-transfer/#gatys-style-transfer","title":"Gatys' Style Transfer","text":"<p>Keras implementation of Style Transfer, with several improvements from recent papers. Has a Google Colaboratory script to use the scripts on GPU's available in the cloud.</p> <ul> <li>Neural Style Transfer</li> </ul>"},{"location":"projects/dl/style-transfer/#windows-application-for-gatys-style-transfer","title":"Windows Application for Gatys' Style Transfer","text":"<p>Windows Form application written in C# to allow easy changing of Neural Style Transfer scripts.</p> <ul> <li>Windows Forms - Style Transfer Application</li> </ul>"},{"location":"projects/dl/style-transfer/#fast-style-transfer","title":"Fast Style Transfer","text":"<p>Unstable implementation of Feed-Forward Style Transfer in Keras.</p> <ul> <li>Fast Style Transfer</li> </ul>"},{"location":"projects/dl/super-resolution/","title":"Neural Image Super-Resolution","text":""},{"location":"projects/dl/super-resolution/#image-super-resolution","title":"Image Super-Resolution","text":"<p>Implementation of Image Super Resolution CNNs in Keras from the paper <code>Image Super-Resolution Using Deep Convolutional Networks</code>.</p> <p>Also contains a modular framework, which allows a variety of other super resolution models to be trained and distilled :</p> <p>1) <code>Denoising Autoencoder SR models</code>  2) <code>ResNet SR models</code>  3) <code>Efficient SubPixel Convolutional SR models</code>  4) <code>Distilled ResNet SR models</code>  5) <code>Non-Local ResNet SR models</code> (experimental) </p> <ul> <li>Image Super-Resolution</li> </ul>"},{"location":"projects/dl/super-resolution/#image-super-resolution-using-gans","title":"Image Super-Resolution using GANs","text":"<p>An incomplete project that attempts to implement the SRGAN model proposed in the paper <code>Photo-Realistic Single  Image Super-Resolution Using a Generative Adversarial Network</code> in Keras.</p> <ul> <li>Super-Resolution using Generative Adversarial Networks</li> </ul>"},{"location":"projects/dl/tensorflow/","title":"Tensorflow","text":"<p>Projects done in Tensorflow - either using Graph Execution or Eager Execution.</p>"},{"location":"projects/dl/tensorflow/#tensorflow-ode-solver","title":"Tensorflow ODE Solver","text":"<p>A library built to replicate the TorchDiffEq library built for the Neural Ordinary Differential Equations paper by Chen et al, running entirely on Tensorflow Eager Execution.</p> <p>All credits for the codebase go to @rtqichen for providing an excellent base to reimplement from.</p> <p>Similar to the PyTorch codebase, this library provides ordinary differential equation (ODE) solvers implemented in Tensorflow Eager. For usage of ODE solvers in deep learning applications, see Neural Ordinary Differential Equations paper.</p> <p>As the solvers are implemented in Tensorflow, algorithms in this repository are fully supported to run on the GPU.</p>"},{"location":"projects/dl/tensorflow/#pyshac-sequential-halving-and-classification","title":"PySHAC - Sequential Halving and Classification","text":"<p>PySHAC is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease.</p>"},{"location":"projects/dl/time-series-classification/","title":"Keras Time Series Classifiers / Recurrent Nets","text":"<p>Scripts which provide a large number of custom Recurrent Neural Network implementations, which can be dropin replaced for LSTM or GRUs. All of these models are built in Keras or Tensorflow.</p>"},{"location":"projects/dl/time-series-classification/#lstm-fully-convolutional-networks","title":"LSTM Fully Convolutional Networks","text":"<p>LSTM FCN models, from the paper LSTM Fully Convolutional Networks for Time Series Classification, augment the fast classification performance of Temporal Convolutional layers with the precise classification of Long Short Term Memory Recurrent Neural Networks.</p>"},{"location":"projects/dl/time-series-classification/#multivariate-lstm-fully-convolutional-networks","title":"Multivariate LSTM Fully Convolutional Networks","text":"<p>MLSTM FCN models, from the paper Multivariate LSTM-FCNs for Time Series Classification, augment the squeeze and excitation block with the state of the art univariate time series model, LSTM-FCN and ALSTM-FCN from the paper LSTM Fully Convolutional Networks for Time Series Classification.</p>"},{"location":"projects/dl/time-series-classification/#chrono-lstm-just-another-neural-network-janet","title":"Chrono LSTM / Just Another Neural Network (JANET)","text":"<p>Keras implementation of the paper The unreasonable effectiveness of the forget gate and the Chrono initializer and Chrono LSTM from the paper Can Recurrent Neural Networks Warp Time?.</p> <p>This model utilizes just 2 gates - forget (f) and context \u00a9 gates out of the 4 gates in a regular LSTM RNN, and uses Chrono Initialization to acheive better performance than regular LSTMs while using fewer parameters and less complicated gating structure.</p>"},{"location":"projects/dl/time-series-classification/#independent-rnn-indrnn","title":"Independent RNN (IndRNN)","text":"<p>Keras implementation of the IndRNN model from the paper Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN.</p>"},{"location":"projects/dl/time-series-classification/#simple-recurrent-unit-sru","title":"Simple Recurrent Unit (SRU)","text":"<p>Implementation of Simple Recurrent Unit in Keras. Paper - Training RNNs as Fast as CNNs.</p> <ul> <li>This is a naive implementation with some speed gains over the generic LSTM cells, however its speed is not yet 10x that of cuDNN LSTMs</li> </ul>"},{"location":"projects/dl/time-series-classification/#nested-lstms","title":"Nested LSTMs","text":"<p>Keras implementation of Nested LSTMs from the paper Nested LSTMs.</p> <p>Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own inner memory cell. Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.</p>"},{"location":"projects/dl/time-series-classification/#multiplicative-lstms","title":"Multiplicative LSTMs","text":"<p>Implementation of the paper Multiplicative LSTM for sequence modelling for Keras 2.0+.</p> <p>Multiplicative LSTMs have been shown to achieve state-of-the-art or close to SotA results for sequence modelling datasets. They also perform better than stacked LSTM models for the Hutter-prize dataset and the raw wikipedia dataset.</p>"},{"location":"projects/dl/time-series-classification/#minimal-rnn","title":"Minimal RNN","text":"<p>Keras implementation of MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks.</p>"},{"location":"research/medical_image_segmentation/diabetic_retinopathy/","title":"Medical Image Segmentation","text":""},{"location":"research/medical_image_segmentation/diabetic_retinopathy/#microaneurysm-detection-using-fully-convolutional-neural-networks","title":"Microaneurysm detection using fully convolutional neural networks","text":"<p>Abstract</p> <p>Backround and Objectives. Diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if  treated not early enough. Microaneurysms are the earliest clinical signs of diabetic retinopathy.  This paper presents an automatic method for detecting microaneurysms in fundus photographies.  Methods A novel patch-based fully convolutional neural network with batch normalization layers and Dice  loss function is proposed. Compared to other methods that require up to five processing stages,  it requires only three. Furthermore, to the best of the authors\u2019 knowledge, this is the first paper  that shows how to successfully transfer knowledge between datasets in the microaneurysm detection domain.  Results The proposed method was evaluated using three publicly available and widely used datasets: E-Ophtha, DIARETDB1, and ROC. It achieved better results than state-of-the-art methods using the FROC metric.  The proposed algorithm accomplished highest sensitivities for low false positive rates, which is  particularly important for screening purposes.</p>"},{"location":"research/medical_image_segmentation/diabetic_retinopathy/#exudate-segmentation-using-fully-convolutional-neural-networks-and-inception-modules","title":"Exudate segmentation using fully convolutional neural networks and inception modules","text":"<p>Abstract</p> <p>Diabetic retinopathy is an eye disease associated with diabetes mellitus and also it is the leading cause of preventable blindness in working-age population. Early detection and treatment of DR is essential to prevent  vision loss. Exudates are one of the earliest signs of diabetic retinopathy. This paper proposes an automatic method for the detection and segmentation of exudates in fundus photographies. A novel fully convolutional  neural network architecture with Inception modules is proposed. Compared to other methods it does not require the removal of other anatomical structures. Furthermore, a transfer learning approach is applied between small datasets of different modalities from the same domain. To the best of authors\u2019 knowledge, it is the first time that such approach has been used in the exudate segmentation domain. The proposed method was evaluated using publicly available E-Ophtha datasets. It achieved better results than the state-of-the-art methods in terms of  sensitivity and specificity metrics. The proposed algorithm accomplished better results using a diseased/not diseased evaluation scenario which indicates its applicability for screening purposes. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.</p>"},{"location":"research/medical_image_segmentation/diabetic_retinopathy/#microaneurysm-detection-using-deep-learning-and-interleaved-freezing","title":"Microaneurysm detection using deep learning and interleaved freezing","text":"<p>Abstract</p> <p>Diabetes affects one in eleven adults. Diabetic retinopathy is a microvascular complication of diabetes and  the leading cause of blindness in the working-age population. Microaneurysms are the earliest clinical signs of diabetic retinopathy. This paper proposes an automatic method for detecting microaneurysms in fundus  photographies. A novel patch-based fully convolutional neural network for detection of microaneurysms is proposed. Compared to other methods that require five processing stages, it requires only two. Furthermore, a novel network fine-tuning scheme called Interleaved Freezing is presented. This procedure significantly  reduces the amount of time needed to re-train a network and produces competitive results. The proposed method was evaluated using publicly available and widely used datasets: E-Ophtha and ROC. It outperforms the  state-of-the-art methods in terms of free-response receiver operatic characteristic (FROC) metric. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic  retinopathy screening applications.</p>"},{"location":"research/misc/misc/","title":"Journal Publications (Algorithms Research / Machine Learning)","text":""},{"location":"research/misc/misc/#parallel-quick-sort-using-thread-pool-pattern","title":"Parallel Quick Sort using Thread Pool Pattern","text":"<p>Abstract</p> <p>Sorting algorithms, their implementations and their applications in modern computing necessitates improvements for sorting large data sets quickly and efficiently. This paper will analyze the performance of a multi-threaded quick sort implemented using the thread pool pattern. The analysis will be done by comparing the time required to sort various data sets and their memory constraints, against the native sorting implementations of the Dual Pivot Quicksort and Merge Sort using the Fork-Join framework in the Oracle Java 8 programming language. Analysis is done of the effect of different number of processor (cores) of the test machine, as well as the performance barrier due to the initial time taken to create \u201cp\u201d threads, p being the number of processors. This paper also analyzes the limitations of the inbuilt Java method Arrays.parallelSort() and how the proposed system overcomes this problem. Finally, it also discuss possible improvements to the proposed system to further improve its performance.</p>"},{"location":"research/misc/misc/#adasort-adaptive-sorting-using-machine-learning","title":"AdaSort: Adaptive Sorting using Machine Learning","text":"<p>Abstract</p> <p>Sorting algorithms and their implementations in modern computing requires improvements in sorting large data sets effectively, both with respect to time and memory consumed. This paper is aimed at reviewing multiple adaptive sorting algorithms, on the basis of selection of an algorithm based on the characteristics of the data set. Machine Learning allows us to construct an adaptive algorithm based on the analysis of the experimental data. A review of algorithms designed using Systems of Algorithmic Algebra and Genetic Algorithms was performed. Both methods are designed to target different use cases. Systems of Algorithmic Algebra is a representation of pseudo code that can be converted to high level code using Integrated toolkit for Design and Synthesis of programs, while the Genetic Algorithm attempts to optimize its fitness function and generate the most successful algorithm.</p>"},{"location":"research/misc/misc/#arxiv-pre-prints","title":"Arxiv Pre-prints","text":""},{"location":"research/misc/misc/#a-comprehensive-comparison-between-neural-style-transfer-and-universal-style-transfer","title":"A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer","text":"<p>Abstract</p> <p>Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: neural style transfer with improvements and universal style transfer. We also make a comparison between the different images produced and how they can be qualitatively measured.</p>"},{"location":"research/speech/asr/","title":"Automatic Speech Recognition","text":""},{"location":"research/speech/asr/#citrinet-closing-the-gap-between-non-autoregressive-and-autoregressive-end-to-end-models-for-automatic-speech-recognition","title":"Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition","text":"<p>Abstract</p> <p>We propose Citrinet - a new end-to-end convolutional Connectionist Temporal Classification (CTC) based  automatic speech recognition (ASR) model. Citrinet is deep residual neural model which uses 1D time-channel  separable convolutions combined with sub-word encoding and squeeze-and-excitation. The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models.  We evaluate Citrinet on LibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English speech  datasets. Citrinet accuracy on these datasets is close to the best autoregressive Transducer models</p>"},{"location":"research/speech/asr/#multi-blank-transducers-for-speech-recognition","title":"Multi-blank Transducers for Speech Recognition","text":"<p>Abstract</p> <p>This paper proposes a modification to RNN-Transducer (RNN-T) models for automatic speech recognition (ASR).  In standard RNN-T, the emission of a blank symbol consumes exactly one input frame; in our proposed method,  we introduce additional blank symbols, which consume two or more input frames when emitted. We refer to the  added symbols as big blanks, and the method multi-blank RNN-T. For training multi-blank RNN-Ts, we propose a  novel logit under-normalization method in order to prioritize emissions of big blanks. With experiments on  multiple languages and datasets, we show that multi-blank RNN-T methods could bring relative speedups of  over +90%/+139% to model inference for English Librispeech and German Multilingual Librispeech datasets,  respectively. The multi-blank RNN-T method also improves ASR accuracy consistently. We will release our  implementation of the method in the NeMo (\\url{https://github.com/NVIDIA/NeMo}) toolkit.</p>"},{"location":"research/speech/asr/#spgispeech-5000-hours-of-transcribed-financial-audio-for-fully-formatted-end-to-end-speech-recognition","title":"SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition","text":"<p>Abstract</p> <p>In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: endto-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for noncommercial use.1</p>"},{"location":"research/speech/asr/#damage-control-during-domain-adaptation-for-transducer-based-automatic-speech-recognition","title":"Damage Control During Domain Adaptation for Transducer Based Automatic Speech Recognition","text":"<p>Abstract</p> <p>Automatic speech recognition models are often adapted to improve their accuracy in a new domain. A potential  drawback of model adaptation to new domains is catastrophic forgetting, where the Word Error Rate on the  original domain is significantly degraded. This paper addresses the situation when we want to simultaneously  adapt automatic speech recognition models to a new domain and limit the degradation of accuracy on the original  domain without access to the original training dataset. We propose several techniques such as a limited  training strategy and regularized adapter modules for the Transducer encoder, prediction, and joiner network.  We apply these methods to the Google Speech Commands and to the UK and Ireland English Dialect speech data set  and obtain strong results on the new target domain while limiting the degradation on the original domain.</p>"},{"location":"research/speech/asr/#carnelinet-neural-mixture-model-for-automatic-speech-recognition","title":"CarneliNet: Neural Mixture Model for Automatic Speech Recognition","text":"<p>Abstract</p> <p>End-to-end automatic speech recognition systems have achieved great accuracy by using deeper and deeper  models. However, the increased depth comes with a larger receptive field that can negatively impact model  performance in streaming scenarios. We propose an alternative approach that we call Neural Mixture Model.  The basic idea is to introduce a parallel mixture of shallow networks instead of a very deep network. To  validate this idea we design CarneliNet -- a CTC-based neural network composed of three mega-blocks. Each mega-block consists of multiple parallel shallow sub-networks based on 1D depthwise-separable convolutions. We evaluate the model on LibriSpeech, MLS and AISHELL-2 datasets and achieved close to state-of-the-art  results for CTC-based models. Finally, we demonstrate that one can dynamically reconfigure the number of parallel sub-networks to accommodate the computational requirements without retraining.</p>"},{"location":"research/speech/asr/#ctc-variations-through-new-wfst-topologies","title":"CTC Variations Through New WFST Topologies","text":"<p>Abstract</p> <p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies to implement Connectionist Temporal Classification (CTC)-like algorithms for automatic speech recognition. Three new CTC variants are proposed: (1) the \"compact-CTC\", in which direct transitions between units are replaced with   back-off transitions; (2) the \"minimal-CTC\", that only adds  self-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants, which disallows self-loop for non-blank units.  Compact-CTC allows for 1.5 times smaller WFST decoding graphs and reduces memory consumption by two  times when training CTC models with the LF-MMI objective without hurting the recognition accuracy.  Minimal-CTC reduces graph size and memory consumption by two and four times for the cost of a small  accuracy drop. Using selfless-CTC can improve the accuracy for wide context window models."},{"location":"research/speech/asr/#improving-noise-robustness-of-an-end-to-end-neural-model-for-automatic-speech-recognition","title":"Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition","text":"<p>Abstract</p> <p>We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR)  model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to  improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions,  especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to  both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With  this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All  described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.</p>"},{"location":"research/speech/asr/#nvidia-nemo-offline-speech-translation-systems-for-iwslt-2022","title":"NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2022","text":"<p>Abstract</p> <p>This paper provides an overview of NVIDIA NeMo\u2019s speech translation systems for the IWSLT 2022 Offline Speech  Translation Task. Our cascade system consists of 1) Conformer RNN-T automatic speech recognition model, 2)  punctuation-capitalization model based on pre-trained T5 encoder, 3) ensemble of Transformer neural machine  translation models fine-tuned on TED talks. Our end-to-end model has less parameters and consists of Conformer  encoder and Transformer decoder. It relies on the cascade system by re-using its pre-trained ASR encoder and  training on synthetic translations generated with the ensemble of NMT models. Our En-&gt;De cascade and end-to-end  systems achieve 29.7 and 26.2 BLEU on the 2020 test set correspondingly, both outperforming the previous year\u2019s  best of 26 BLEU.</p>"},{"location":"research/speech/speech_classification/","title":"Speech Classification","text":""},{"location":"research/speech/speech_classification/#matchboxnet-1d-time-channel-separable-convolutional-neural-network-architecture-for-speech-commands-recognition","title":"MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition","text":"<p>Abstract</p> <p>We present MatchboxNet \u2014 an end-to-end neural network for speech command recognition. MatchboxNet is a  deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. MatchboxNet reaches state-of-the art accuracy on the Google Speech Commands  dataset while having significantly fewer parameters than similar models. The small footprint of  MatchboxNet makes it an attractive candidate for devices with limited computational resources. The model is highly scalable, so model accuracy can be improved with modest additional memory and compute. Finally, we show how intensive data augmentation using an auxiliary noise dataset improves robustness in the presence of background noise.</p>"},{"location":"research/speech/speech_classification/#marblenet-deep-1d-time-channel-separable-convolutional-neural-network-for-voice-activity","title":"MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity","text":"<p>Abstract</p> <p>We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet  is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve  similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on  different training methods and choices of parameters in order to study the robustness of MarbleNet in  real-world VAD tasks.</p>"},{"location":"research/time_series_classification/tsc/","title":"Time Series Classification","text":""},{"location":"research/time_series_classification/tsc/#lstm-fully-convolutional-networks-for-time-series-classification","title":"LSTM Fully Convolutional Networks for Time Series Classification","text":"<p>Abstract</p> <p>Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance  on the task of classifying time series sequences. We propose the augmentation of fully convolutional  networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series  classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set.  The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The  attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose  refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques.</p>"},{"location":"research/time_series_classification/tsc/#multivariate-lstm-fcns-for-time-series-classification","title":"Multivariate LSTM-FCNs for Time Series Classification","text":"<p>Abstract</p> <p>Over the past decade, multivariate time series classification has been receiving a lot of attention.  We propose augmenting the existing univariate time series classification models, LSTM-FCN and ALSTM-FCN  with a squeeze and excitation block to further improve performance. Our proposed models outperform most of the state of the art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.</p>"},{"location":"research/time_series_classification/tsc/#insights-into-lstm-fully-convolutional-networks-for-time-series-classification","title":"Insights Into LSTM Fully Convolutional Networks for Time Series Classification","text":"<p>Abstract</p> <p>Long short-term memory fully convolutional neural networks (LSTM-FCNs) and Attention LSTM-FCN (ALSTM-FCN) have shown to achieve the state-of-the-art performance on the task of classifying time series signals on the old University of California-Riverside (UCR) time series repository. However, there has been no study on why LSTM-FCN and ALSTM-FCN perform well. In this paper, we perform a series of ablation tests (3627 experiments) on the LSTM-FCN and ALSTM-FCN to provide a better understanding of the model and each  of its sub-modules. The results from the ablation tests on the ALSTM-FCN and LSTM-FCN show that the LSTM  and the FCN blocks perform better when applied in a conjoined manner. Two z-normalizing techniques,  z-normalizing each sample independently and z-normalizing the whole dataset, are compared using a  Wilcoxson signed-rank test to show a statistical difference in performance. In addition, we provide an  understanding of the impact dimension shuffle that has on LSTM-FCN by comparing its performance with LSTM-FCN when no dimension shuffle is applied. Finally, we demonstrate the performance of the LSTM-FCN when the LSTM  block is replaced by a gated recurrent unit (GRU), basic neural network (RNN), and dense block.</p>"},{"location":"research/time_series_classification/tsc/#adversarial-attacks-on-time-series","title":"Adversarial Attacks on Time Series","text":"<p>Abstract</p> <p>Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These  adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial  transformation network (ATN) on a distilled model to attack various time series classification models.  The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics  the behavior of the attacked classical time series classification models. Our proposed methodology is  applied onto 1-nearest neighbor dynamic time warping (1-NN DTW) and a fully convolutional network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. When compared to Fast Gradient Sign Method, the proposed attack generates a larger faction of successful adversarial black-box attacks. A simple  defense mechanism is successfully devised to reduce the fraction of successful adversarial samples. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples.</p>"}]}