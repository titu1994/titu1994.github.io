{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Me \u00b6 I'm a Deep Learning and Machine Learning Research enthusiast, currently attempting to implement and analyze various papers in the field of Artificial Intelligence. For me, research needs to be distributed to as many sources as possible, so that practical applications from that work can be applied in the field, and is available to as many as possible. I enjoy working on Image Classification and many of its sub-domains, Time Series Classification and have recently started to expand my horizons further - architecture search, optimizers, mobile computing and more.","title":"Info"},{"location":"#about-me","text":"I'm a Deep Learning and Machine Learning Research enthusiast, currently attempting to implement and analyze various papers in the field of Artificial Intelligence. For me, research needs to be distributed to as many sources as possible, so that practical applications from that work can be applied in the field, and is available to as many as possible. I enjoy working on Image Classification and many of its sub-domains, Time Series Classification and have recently started to expand my horizons further - architecture search, optimizers, mobile computing and more.","title":"About Me"},{"location":"academia/","text":"Education \u00b6 Master of Science in Computer Science \u00b6 University of Illinois at Chicago : 3.77 / 4.0 2017-2018, Chicago, IL, USA Bachelor of Engineering in Computer Science \u00b6 D.J. Sanghvi College of Engineering : 8.0 / 10.0 2012-2016, Mumbai, Maharashtra, India","title":"Academia"},{"location":"academia/#education","text":"","title":"Education"},{"location":"academia/#master-of-science-in-computer-science","text":"University of Illinois at Chicago : 3.77 / 4.0 2017-2018, Chicago, IL, USA","title":"Master of Science in Computer Science"},{"location":"academia/#bachelor-of-engineering-in-computer-science","text":"D.J. Sanghvi College of Engineering : 8.0 / 10.0 2012-2016, Mumbai, Maharashtra, India","title":"Bachelor of Engineering in Computer Science"},{"location":"contact/","text":"Contact information: Resume : PDF Github : titu1994 LinkedIn : https://www.linkedin.com/in/somshubramajumdar/ Twitter : @haseoX94 Mail : titu1994@gmail.com","title":"Contact"},{"location":"projects/android/","text":"Android Projects \u00b6 Ragial Notifier \u00b6 Android application that connects to Ragial , the online browser to International Ragnarok Online's market place, and notifies the user of when marked items are being sold at a sale. Ragial Searcher \u00b6 The java core library built to query and parse Ragial for details about products.","title":"Android"},{"location":"projects/android/#android-projects","text":"","title":"Android Projects"},{"location":"projects/android/#ragial-notifier","text":"Android application that connects to Ragial , the online browser to International Ragnarok Online's market place, and notifies the user of when marked items are being sold at a sale.","title":"Ragial Notifier"},{"location":"projects/android/#ragial-searcher","text":"The java core library built to query and parse Ragial for details about products.","title":"Ragial Searcher"},{"location":"projects/dl/additional/","text":"Various Projects \u00b6 Keras LAMB Optimizer \u00b6 Implementation of the LAMB optimizer from the paper Reducing BERT Pre-Training Time from 3 Days to 76 Minutes . Supports huge batch size training while only requiring learning rate to be changed. Keras AdaBound Optimizer \u00b6 Keras port of AdaBound Optimizer for PyTorch, from the paper Adaptive Gradient Methods with Dynamic Bound of Learning Rate . Provides weights for ResNet 34 model trained on CIFAR 10 reaching 92% without random cropping augmentation. Dynamic Time Warping - Numba \u00b6 Implementation of Dynamic Time Warping algorithm with speed improvements based on Numba. Supports for K nearest neighbours classifier using Dynamic Time Warping, based on the work presented by Mark Regan. The classes called KnnDTW are obtained from there, as a simplified interface akin to Scikit-Learn. The three variants available are in dtw.py, odtw.py and ucrdtw.py. dtw.py: Single threaded variant, support for visualizing the progress bar. odtw.py: Multi threaded variant, no support for visualization. In practice, much more effiecient. ucrdtw.py: Experimental (Do not use). Multi threaded variant, no support for visualization. It is based upon the optimized C implementation available at https://github.com/klon/ucrdtw. Neural Algorithmic Logic Units \u00b6 A Keras implementation of Neural Arithmatic and Logical Unit from the paper Neural Algorithmic Logic Units by Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, Phil Blunsom. Contains the layers for Neural Arithmatic Logic Unit (NALU) and Neural Accumulator (NAC) . Also contains the results of the static function learning toy tests. Padam - Partially Adaptive Momentum Estimation \u00b6 Keras implementation of Padam from Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks . Padam allows for much larger learning rates to be utilized, and follows generalization closely with Stochastc Gradient Descent. Neural Image Assessment \u00b6 Implementation of NIMA: Neural Image Assessment in Keras + Tensorflow with weights for MobileNet model trained on AVA dataset. NIMA assigns a Mean + Standard Deviation score to images, and can be used as a tool to automatically inspect quality of images or as a loss function to further improve the quality of generated images. Contains weights trained on the AVA dataset for the following models: NASNet Mobile (0.067 EMD on valset thanks to @tfriedel !, 0.0848 EMD with just pre-training) Inception ResNet v2 (~ 0.07 EMD on valset, thanks to @tfriedel !) MobileNet (0.0804 EMD on valset) Switch Normalization \u00b6 Switchable Normalization is a normalization technique that is able to learn different normalization operations for different normalization layers in a deep neural network in an end-to-end manner. Keras port of the implementation of the paper Differentiable Learning-to-Normalize via Switchable Normalization . Code ported from the switchnorm official repository . Group Normalization \u00b6 A Keras implementation of Group Normalization by Yuxin Wu and Kaiming He. Useful for fine-tuning of large models on smaller batch sizes than in research setting (where batch size is very large due to multiple GPUs). Similar to Batch Renormalization, but performs significantly better on ImageNet. Available in Keras Contrib inside normalization module. Batch Renormalization \u00b6 Batch Renormalization algorithm implementation in Keras 2.0+. Original paper by Sergey Ioffe, Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models . Available in Keras Contrib inside normalization module. Snapshot Ensembles \u00b6 Implementation of the paper Snapshot Ensembles: Train 1, Get M for Free in Keras 2+ Tiramisu DenseNets for Semantic Segmentation \u00b6 Fully Connected DenseNet for Image Segmentation implementation of the paper The One Hundred Layers Tiramisu : Fully Convolutional DenseNets for Semantic Segmentation Available in Keras Contrib inside the DenseNet applications module. One Cycle Learning Policy \u00b6 Implementation of One-Cycle Learning rate policy from the papers by Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates Contains two Keras callbacks, LRFinder and OneCycleLR which are ported from the PyTorch Fast.ai library. Normalized Optimizers \u00b6 Keras wrapper class for Normalized Gradient Descent from kmkolasinski/max-normed-optimizer, which can be applied to almost all Keras optimizers. Partially implements Block-Normalized Gradient Method: An Empirical Study for Training Deep Neural Network for all base Keras optimizers, and allows flexibility to choose any normalizing function. It does not implement adaptive learning rates however. The wrapper class can also be extended to allow Gradient Masking and Gradient Clipping using custom norm metrics. Wrapper classes : NormalizedOptimizer: To normalize of gradient by the norm of that gradient. ClippedOptimizer: To clip the gradient by the norm of that gradient. Note: Clips by Local Norm only ! Mobile Colorizer \u00b6 Utilizes a U-Net inspired model conditioned on MobileNet class features to generate a mapping from Grayscale to Color image. Based on the work https://github.com/baldassarreFe/deep-koalarization Uses MobileNets for memory efficiency in comparison to Inception-ResNet-V2 so that training can be done on a single GPU (of 4 GB size minimum). Tensorflow Eager Execution Examples \u00b6 Tensorflow Eager Execution mode allows an imperative programming style, similar to Numpy in addition to nearly all of the Tensorflow graph APIs, higher level APIs to build models (Keras) as well as easy debugging with the Python debug bridge. Since Eager Execution APIs are quite recent, some kinks still exist, but as of this moment, they are minor and can be sidesteped. These issues are highlighted in the notebooks and it is advised to browse through the comments, even if the topic is easy, so as to understand the limitations of Eager as TF 1.8. The following set of examples show usage of higher level APIs of Keras, different ways of performing the same thing, some issues that can arise and how to sidestep them while we wait for updates in Tensorflow to fix them. It is to be noted, that I try to replicate most parts of this excellent PyTorch Tutorial Set. A few topics are missing - such as GANs and Image Captioning since I do not have the computational resources to train such models. A notable exception is Style Transfer, for which I have another repository dedicated to it, so I won't be porting it to Eager. A final note : Eager is evolving rapidly, and almost all of these issues that I stated here are edge cases that can/will be resolved in a later update. I still appreciate Eager, even with its limitations, as it offers a rich set of APIs from its Tensorflow heritage in an imperative execution environment like PyTorch. This means that once the Eager API has all of its kinks ironed out, it will result in cleaner, more concise code and hopefully at performance close to Tensorflow itself. Twitter Sentiment Analysis \u00b6 To perform sentiment analysis over a corpus of tweets during the U.S. 2012 Re-Election about the candidates Barack Obama and Mitt Romney. The previous best score on the test dataset was 64 % f1-score, suggesting that improvements can be obtained using modern machine learning / deep learning algorithms.","title":"Additional Work"},{"location":"projects/dl/additional/#various-projects","text":"","title":"Various Projects"},{"location":"projects/dl/additional/#keras-lamb-optimizer","text":"Implementation of the LAMB optimizer from the paper Reducing BERT Pre-Training Time from 3 Days to 76 Minutes . Supports huge batch size training while only requiring learning rate to be changed.","title":"Keras LAMB Optimizer"},{"location":"projects/dl/additional/#keras-adabound-optimizer","text":"Keras port of AdaBound Optimizer for PyTorch, from the paper Adaptive Gradient Methods with Dynamic Bound of Learning Rate . Provides weights for ResNet 34 model trained on CIFAR 10 reaching 92% without random cropping augmentation.","title":"Keras AdaBound Optimizer"},{"location":"projects/dl/additional/#dynamic-time-warping-numba","text":"Implementation of Dynamic Time Warping algorithm with speed improvements based on Numba. Supports for K nearest neighbours classifier using Dynamic Time Warping, based on the work presented by Mark Regan. The classes called KnnDTW are obtained from there, as a simplified interface akin to Scikit-Learn. The three variants available are in dtw.py, odtw.py and ucrdtw.py. dtw.py: Single threaded variant, support for visualizing the progress bar. odtw.py: Multi threaded variant, no support for visualization. In practice, much more effiecient. ucrdtw.py: Experimental (Do not use). Multi threaded variant, no support for visualization. It is based upon the optimized C implementation available at https://github.com/klon/ucrdtw.","title":"Dynamic Time Warping - Numba"},{"location":"projects/dl/additional/#neural-algorithmic-logic-units","text":"A Keras implementation of Neural Arithmatic and Logical Unit from the paper Neural Algorithmic Logic Units by Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, Phil Blunsom. Contains the layers for Neural Arithmatic Logic Unit (NALU) and Neural Accumulator (NAC) . Also contains the results of the static function learning toy tests.","title":"Neural Algorithmic Logic Units"},{"location":"projects/dl/additional/#padam-partially-adaptive-momentum-estimation","text":"Keras implementation of Padam from Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks . Padam allows for much larger learning rates to be utilized, and follows generalization closely with Stochastc Gradient Descent.","title":"Padam - Partially Adaptive Momentum Estimation"},{"location":"projects/dl/additional/#neural-image-assessment","text":"Implementation of NIMA: Neural Image Assessment in Keras + Tensorflow with weights for MobileNet model trained on AVA dataset. NIMA assigns a Mean + Standard Deviation score to images, and can be used as a tool to automatically inspect quality of images or as a loss function to further improve the quality of generated images. Contains weights trained on the AVA dataset for the following models: NASNet Mobile (0.067 EMD on valset thanks to @tfriedel !, 0.0848 EMD with just pre-training) Inception ResNet v2 (~ 0.07 EMD on valset, thanks to @tfriedel !) MobileNet (0.0804 EMD on valset)","title":"Neural Image Assessment"},{"location":"projects/dl/additional/#switch-normalization","text":"Switchable Normalization is a normalization technique that is able to learn different normalization operations for different normalization layers in a deep neural network in an end-to-end manner. Keras port of the implementation of the paper Differentiable Learning-to-Normalize via Switchable Normalization . Code ported from the switchnorm official repository .","title":"Switch Normalization"},{"location":"projects/dl/additional/#group-normalization","text":"A Keras implementation of Group Normalization by Yuxin Wu and Kaiming He. Useful for fine-tuning of large models on smaller batch sizes than in research setting (where batch size is very large due to multiple GPUs). Similar to Batch Renormalization, but performs significantly better on ImageNet. Available in Keras Contrib inside normalization module.","title":"Group Normalization"},{"location":"projects/dl/additional/#batch-renormalization","text":"Batch Renormalization algorithm implementation in Keras 2.0+. Original paper by Sergey Ioffe, Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models . Available in Keras Contrib inside normalization module.","title":"Batch Renormalization"},{"location":"projects/dl/additional/#snapshot-ensembles","text":"Implementation of the paper Snapshot Ensembles: Train 1, Get M for Free in Keras 2+","title":"Snapshot Ensembles"},{"location":"projects/dl/additional/#tiramisu-densenets-for-semantic-segmentation","text":"Fully Connected DenseNet for Image Segmentation implementation of the paper The One Hundred Layers Tiramisu : Fully Convolutional DenseNets for Semantic Segmentation Available in Keras Contrib inside the DenseNet applications module.","title":"Tiramisu DenseNets for Semantic Segmentation"},{"location":"projects/dl/additional/#one-cycle-learning-policy","text":"Implementation of One-Cycle Learning rate policy from the papers by Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates Contains two Keras callbacks, LRFinder and OneCycleLR which are ported from the PyTorch Fast.ai library.","title":"One Cycle Learning Policy"},{"location":"projects/dl/additional/#normalized-optimizers","text":"Keras wrapper class for Normalized Gradient Descent from kmkolasinski/max-normed-optimizer, which can be applied to almost all Keras optimizers. Partially implements Block-Normalized Gradient Method: An Empirical Study for Training Deep Neural Network for all base Keras optimizers, and allows flexibility to choose any normalizing function. It does not implement adaptive learning rates however. The wrapper class can also be extended to allow Gradient Masking and Gradient Clipping using custom norm metrics. Wrapper classes : NormalizedOptimizer: To normalize of gradient by the norm of that gradient. ClippedOptimizer: To clip the gradient by the norm of that gradient. Note: Clips by Local Norm only !","title":"Normalized Optimizers"},{"location":"projects/dl/additional/#mobile-colorizer","text":"Utilizes a U-Net inspired model conditioned on MobileNet class features to generate a mapping from Grayscale to Color image. Based on the work https://github.com/baldassarreFe/deep-koalarization Uses MobileNets for memory efficiency in comparison to Inception-ResNet-V2 so that training can be done on a single GPU (of 4 GB size minimum).","title":"Mobile Colorizer"},{"location":"projects/dl/additional/#tensorflow-eager-execution-examples","text":"Tensorflow Eager Execution mode allows an imperative programming style, similar to Numpy in addition to nearly all of the Tensorflow graph APIs, higher level APIs to build models (Keras) as well as easy debugging with the Python debug bridge. Since Eager Execution APIs are quite recent, some kinks still exist, but as of this moment, they are minor and can be sidesteped. These issues are highlighted in the notebooks and it is advised to browse through the comments, even if the topic is easy, so as to understand the limitations of Eager as TF 1.8. The following set of examples show usage of higher level APIs of Keras, different ways of performing the same thing, some issues that can arise and how to sidestep them while we wait for updates in Tensorflow to fix them. It is to be noted, that I try to replicate most parts of this excellent PyTorch Tutorial Set. A few topics are missing - such as GANs and Image Captioning since I do not have the computational resources to train such models. A notable exception is Style Transfer, for which I have another repository dedicated to it, so I won't be porting it to Eager. A final note : Eager is evolving rapidly, and almost all of these issues that I stated here are edge cases that can/will be resolved in a later update. I still appreciate Eager, even with its limitations, as it offers a rich set of APIs from its Tensorflow heritage in an imperative execution environment like PyTorch. This means that once the Eager API has all of its kinks ironed out, it will result in cleaner, more concise code and hopefully at performance close to Tensorflow itself.","title":"Tensorflow Eager Execution Examples"},{"location":"projects/dl/additional/#twitter-sentiment-analysis","text":"To perform sentiment analysis over a corpus of tweets during the U.S. 2012 Re-Election about the candidates Barack Obama and Mitt Romney. The previous best score on the test dataset was 64 % f1-score, suggesting that improvements can be obtained using modern machine learning / deep learning algorithms.","title":"Twitter Sentiment Analysis"},{"location":"projects/dl/contrib/","text":"Contributions to Keras and Keras Contrib \u00b6 Keras Applications \u00b6 Various state of the art models listed below were merged with Keras or Keras contrib as below : NASNet (Keras) MobilNet V1 (Keras) NASNet (Keras-Contrib) DenseNet (Keras-Contrib) Wide ResNet (Keras-Contrib) Residual of Residual Networks (Keras-Contrib) Contrib Callbacks \u00b6 Added the Snapshot Ensemble callback manager for Contrib. Contrib Layers \u00b6 Added a few layers to Keras Contrib : SubPixelUpscaling BatchRenormalization GroupNormalization","title":"Keras Contrib"},{"location":"projects/dl/contrib/#contributions-to-keras-and-keras-contrib","text":"","title":"Contributions to Keras and Keras Contrib"},{"location":"projects/dl/contrib/#keras-applications","text":"Various state of the art models listed below were merged with Keras or Keras contrib as below : NASNet (Keras) MobilNet V1 (Keras) NASNet (Keras-Contrib) DenseNet (Keras-Contrib) Wide ResNet (Keras-Contrib) Residual of Residual Networks (Keras-Contrib)","title":"Keras Applications"},{"location":"projects/dl/contrib/#contrib-callbacks","text":"Added the Snapshot Ensemble callback manager for Contrib.","title":"Contrib Callbacks"},{"location":"projects/dl/contrib/#contrib-layers","text":"Added a few layers to Keras Contrib : SubPixelUpscaling BatchRenormalization GroupNormalization","title":"Contrib Layers"},{"location":"projects/dl/image-classifiers/","text":"Keras Image Classifiers \u00b6 Model building scripts which replicate the architectures of various state of the art papers. All of these models are built in Keras or Tensorflow. Octave Convolutions \u00b6 Keras implementation of the Octave Convolution blocks from the paper Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution . Provides code blocks for initializing the Oct-Conv architecture, building Oct-Conv blocks and closing the Oct-conv architecture, as well as Octave-Resnet models. Non-Local Neural Networks \u00b6 Keras implementation of Non-local blocks from the paper Non-local Neural Networks . Support for \"Gaussian\", \"Embedded Gaussian\" and \"Dot\" instantiations of the Non-Local block. Support for variable shielded computation mode (reduces computation by N**2 x, where N is default to 2) Support for \"Concatenation\" instantiation will be supported when authors release their code. Squeeze & Excitation Networks \u00b6 Implementation of Squeeze and Excitation Networks , as an independent block that can be added to any Keras layer, or pre-built models such as : SE-ResNet . Custom ResNets can be built using the SEResNet model builder, whereas prebuilt Resnet models such as SEResNet50, SEResNet101 and SEResNet154 can also be built directly. SE-InceptionV3 SE-Inception-ResNet-v2 SE-ResNeXt Additional models (not from the paper, not verified if they improve performance) SE-MobileNets SE-DenseNet - Custom SE-DenseNets can be built using SEDenseNet model builder, whereas prebuilt SEDenseNet models such as SEDenseNetImageNet121, SEDenseNetImageNet169, SEDenseNetImageNet161, SEDenseNetImageNet201 and SEDenseNetImageNet264 can be build DenseNet in ImageNet configuration. To use SEDenseNet in CIFAR mode, use the SEDenseNet model builder. NASNet \u00b6 An implementation of \"NASNet\" models from the paper Learning Transferable Architectures for Scalable Image Recognitio in Keras 2.0+. Based on the models described in the TFSlim implementation and some modules from the TensorNets implementation. Weights have been ported over from the official NASNet Tensorflow repository . MobileNets V1 and V2 \u00b6 Keras implementation of the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications . Contains the Keras implementation of the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks . Weights for all variants of MobileNet V1 and MobileNet V2 are available . SparseNets \u00b6 Keras Implementation of Sparse Networks from the paper Sparsely Connected Convolutional Networks . Code derived from the offical repository - https://github.com/Lyken17/SparseNet No weights available as they are not released . Dual Path Networks \u00b6 Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks . Due to Keras and Tensorflow not supporting Grouped Convolutions yet, this is an inefficient implementation with no weights . ResNeXt \u00b6 Implementation of ResNeXt models from the paper Aggregated Residual Transformations for Deep Neural Networks in Keras 2.0+. Contains code for building the general ResNeXt model (optimized for datasets similar to CIFAR) and ResNeXtImageNet (optimized for the ImageNet dataset). Due to Keras and Tensorflow not supporting Grouped Convolutions yet, this is an inefficient implementation with no weights . Inception v4 / Inception ResNet v2 \u00b6 Implementations of the Inception-v4, Inception - Resnet-v1 and v2 Architectures in Keras using the Functional API. The paper on these architectures is available at Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning . Weights are provided for Inception v4 and Inception ResNet v2 Models . DenseNets \u00b6 DenseNet implementation of the paper Densely Connected Convolutional Networks in Keras Now supports the more efficient DenseNet-BC (DenseNet-Bottleneck-Compressed) networks. Using the DenseNet-BC-190-40 model, it obtaines state of the art performance on CIFAR-10 and CIFAR-100 Weights are provided for DenseNet Models . Wide Residual Networks \u00b6 Implementation of Wide Residual Networks from the paper Wide Residual Networks in Keras. No weights available due to limited computation available . Residual-of-Residual Networks \u00b6 This is an implementation of the paper Residual Networks of Residual Networks: Multilevel Residual Networks","title":"Image Classifiers"},{"location":"projects/dl/image-classifiers/#keras-image-classifiers","text":"Model building scripts which replicate the architectures of various state of the art papers. All of these models are built in Keras or Tensorflow.","title":"Keras Image Classifiers"},{"location":"projects/dl/image-classifiers/#octave-convolutions","text":"Keras implementation of the Octave Convolution blocks from the paper Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution . Provides code blocks for initializing the Oct-Conv architecture, building Oct-Conv blocks and closing the Oct-conv architecture, as well as Octave-Resnet models.","title":"Octave Convolutions"},{"location":"projects/dl/image-classifiers/#non-local-neural-networks","text":"Keras implementation of Non-local blocks from the paper Non-local Neural Networks . Support for \"Gaussian\", \"Embedded Gaussian\" and \"Dot\" instantiations of the Non-Local block. Support for variable shielded computation mode (reduces computation by N**2 x, where N is default to 2) Support for \"Concatenation\" instantiation will be supported when authors release their code.","title":"Non-Local Neural Networks"},{"location":"projects/dl/image-classifiers/#squeeze-excitation-networks","text":"Implementation of Squeeze and Excitation Networks , as an independent block that can be added to any Keras layer, or pre-built models such as : SE-ResNet . Custom ResNets can be built using the SEResNet model builder, whereas prebuilt Resnet models such as SEResNet50, SEResNet101 and SEResNet154 can also be built directly. SE-InceptionV3 SE-Inception-ResNet-v2 SE-ResNeXt Additional models (not from the paper, not verified if they improve performance) SE-MobileNets SE-DenseNet - Custom SE-DenseNets can be built using SEDenseNet model builder, whereas prebuilt SEDenseNet models such as SEDenseNetImageNet121, SEDenseNetImageNet169, SEDenseNetImageNet161, SEDenseNetImageNet201 and SEDenseNetImageNet264 can be build DenseNet in ImageNet configuration. To use SEDenseNet in CIFAR mode, use the SEDenseNet model builder.","title":"Squeeze &amp; Excitation Networks"},{"location":"projects/dl/image-classifiers/#nasnet","text":"An implementation of \"NASNet\" models from the paper Learning Transferable Architectures for Scalable Image Recognitio in Keras 2.0+. Based on the models described in the TFSlim implementation and some modules from the TensorNets implementation. Weights have been ported over from the official NASNet Tensorflow repository .","title":"NASNet"},{"location":"projects/dl/image-classifiers/#mobilenets-v1-and-v2","text":"Keras implementation of the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications . Contains the Keras implementation of the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks . Weights for all variants of MobileNet V1 and MobileNet V2 are available .","title":"MobileNets V1 and V2"},{"location":"projects/dl/image-classifiers/#sparsenets","text":"Keras Implementation of Sparse Networks from the paper Sparsely Connected Convolutional Networks . Code derived from the offical repository - https://github.com/Lyken17/SparseNet No weights available as they are not released .","title":"SparseNets"},{"location":"projects/dl/image-classifiers/#dual-path-networks","text":"Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks . Due to Keras and Tensorflow not supporting Grouped Convolutions yet, this is an inefficient implementation with no weights .","title":"Dual Path Networks"},{"location":"projects/dl/image-classifiers/#resnext","text":"Implementation of ResNeXt models from the paper Aggregated Residual Transformations for Deep Neural Networks in Keras 2.0+. Contains code for building the general ResNeXt model (optimized for datasets similar to CIFAR) and ResNeXtImageNet (optimized for the ImageNet dataset). Due to Keras and Tensorflow not supporting Grouped Convolutions yet, this is an inefficient implementation with no weights .","title":"ResNeXt"},{"location":"projects/dl/image-classifiers/#inception-v4-inception-resnet-v2","text":"Implementations of the Inception-v4, Inception - Resnet-v1 and v2 Architectures in Keras using the Functional API. The paper on these architectures is available at Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning . Weights are provided for Inception v4 and Inception ResNet v2 Models .","title":"Inception v4 / Inception ResNet v2"},{"location":"projects/dl/image-classifiers/#densenets","text":"DenseNet implementation of the paper Densely Connected Convolutional Networks in Keras Now supports the more efficient DenseNet-BC (DenseNet-Bottleneck-Compressed) networks. Using the DenseNet-BC-190-40 model, it obtaines state of the art performance on CIFAR-10 and CIFAR-100 Weights are provided for DenseNet Models .","title":"DenseNets"},{"location":"projects/dl/image-classifiers/#wide-residual-networks","text":"Implementation of Wide Residual Networks from the paper Wide Residual Networks in Keras. No weights available due to limited computation available .","title":"Wide Residual Networks"},{"location":"projects/dl/image-classifiers/#residual-of-residual-networks","text":"This is an implementation of the paper Residual Networks of Residual Networks: Multilevel Residual Networks","title":"Residual-of-Residual Networks"},{"location":"projects/dl/nas/","text":"Neural Architecture Search \u00b6 Utilization of a Controller using Reinforcement Learning, Sequentual Model Based Optimization to train via surrogate losses, or using XGBoost Trees to reduce the search space. Neural Architecture Search \u00b6 Basic and limited (1 GPU) implementation of Controller RNN from Neural Architecture Search with Reinforcement Learning and Learning Transferable Architectures for Scalable Image Recognition . Uses Keras to define and train children / generated networks, which are defined in Tensorflow by the Controller RNN. Define a state space by using StateSpace, a manager which adds states and handles communication between the Controller RNN and the user. Controller manages the training and evaluation of the Controller RNN NetworkManager handles the training and reward computation of a Keras model Progressive Neural Architecture Search \u00b6 Basic and limited (1 GPU) implementation of Encoder RNN from Progressive Neural Architecture Search . Uses Keras to define and train children / generated networks, which are found via sequential model-based optimization in Tensorflow, ranked by the Encoder RNN. Define a state space by using StateSpace, a manager which maintains input states and handles communication between the Encoder RNN and the user. Encoder manages the training and evaluation of the Encoder RNN NetworkManager handles the training and reward computation of the children Keras model Sequentual Halving and Classification \u00b6 PySHAC is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease. Note : This library is not affiliated with Google. Stable build documentation can be found at PySHAC Documentation . It contains a User Guide, as well as explanation of the different engines that can be used with PySHAC.","title":"Neural Architecture Search"},{"location":"projects/dl/nas/#neural-architecture-search","text":"Utilization of a Controller using Reinforcement Learning, Sequentual Model Based Optimization to train via surrogate losses, or using XGBoost Trees to reduce the search space.","title":"Neural Architecture Search"},{"location":"projects/dl/nas/#neural-architecture-search_1","text":"Basic and limited (1 GPU) implementation of Controller RNN from Neural Architecture Search with Reinforcement Learning and Learning Transferable Architectures for Scalable Image Recognition . Uses Keras to define and train children / generated networks, which are defined in Tensorflow by the Controller RNN. Define a state space by using StateSpace, a manager which adds states and handles communication between the Controller RNN and the user. Controller manages the training and evaluation of the Controller RNN NetworkManager handles the training and reward computation of a Keras model","title":"Neural Architecture Search"},{"location":"projects/dl/nas/#progressive-neural-architecture-search","text":"Basic and limited (1 GPU) implementation of Encoder RNN from Progressive Neural Architecture Search . Uses Keras to define and train children / generated networks, which are found via sequential model-based optimization in Tensorflow, ranked by the Encoder RNN. Define a state space by using StateSpace, a manager which maintains input states and handles communication between the Encoder RNN and the user. Encoder manages the training and evaluation of the Encoder RNN NetworkManager handles the training and reward computation of the children Keras model","title":"Progressive Neural Architecture Search"},{"location":"projects/dl/nas/#sequentual-halving-and-classification","text":"PySHAC is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease. Note : This library is not affiliated with Google. Stable build documentation can be found at PySHAC Documentation . It contains a User Guide, as well as explanation of the different engines that can be used with PySHAC.","title":"Sequentual Halving and Classification"},{"location":"projects/dl/style-transfer/","text":"Naural Style Transfer \u00b6 Gatys' Style Transfer \u00b6 Keras implementation of Style Transfer, with several improvements from recent papers. Has a Google Colaboratory script to use the scripts on GPU's available in the cloud. Neural Style Transfer Windows Application for Gatys' Style Transfer \u00b6 Windows Form application written in C# to allow easy changing of Neural Style Transfer scripts. Windows Forms - Style Transfer Application Fast Style Transfer \u00b6 Unstable implementation of Feed-Forward Style Transfer in Keras. Fast Style Transfer","title":"Neural Style Transfer"},{"location":"projects/dl/style-transfer/#naural-style-transfer","text":"","title":"Naural Style Transfer"},{"location":"projects/dl/style-transfer/#gatys-style-transfer","text":"Keras implementation of Style Transfer, with several improvements from recent papers. Has a Google Colaboratory script to use the scripts on GPU's available in the cloud. Neural Style Transfer","title":"Gatys' Style Transfer"},{"location":"projects/dl/style-transfer/#windows-application-for-gatys-style-transfer","text":"Windows Form application written in C# to allow easy changing of Neural Style Transfer scripts. Windows Forms - Style Transfer Application","title":"Windows Application for Gatys' Style Transfer"},{"location":"projects/dl/style-transfer/#fast-style-transfer","text":"Unstable implementation of Feed-Forward Style Transfer in Keras. Fast Style Transfer","title":"Fast Style Transfer"},{"location":"projects/dl/super-resolution/","text":"Neural Image Super-Resolution \u00b6 Image Super-Resolution \u00b6 Implementation of Image Super Resolution CNNs in Keras from the paper Image Super-Resolution Using Deep Convolutional Networks . Also contains a modular framework, which allows a variety of other super resolution models to be trained and distilled : 1) Denoising Autoencoder SR models 2) ResNet SR models 3) Efficient SubPixel Convolutional SR models 4) Distilled ResNet SR models 5) Non-Local ResNet SR models (experimental) Image Super-Resolution Image Super-Resolution using GANs \u00b6 An incomplete project that attempts to implement the SRGAN model proposed in the paper Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network in Keras. Super-Resolution using Generative Adversarial Networks","title":"Image Super-Resolution"},{"location":"projects/dl/super-resolution/#neural-image-super-resolution","text":"","title":"Neural Image Super-Resolution"},{"location":"projects/dl/super-resolution/#image-super-resolution","text":"Implementation of Image Super Resolution CNNs in Keras from the paper Image Super-Resolution Using Deep Convolutional Networks . Also contains a modular framework, which allows a variety of other super resolution models to be trained and distilled : 1) Denoising Autoencoder SR models 2) ResNet SR models 3) Efficient SubPixel Convolutional SR models 4) Distilled ResNet SR models 5) Non-Local ResNet SR models (experimental) Image Super-Resolution","title":"Image Super-Resolution"},{"location":"projects/dl/super-resolution/#image-super-resolution-using-gans","text":"An incomplete project that attempts to implement the SRGAN model proposed in the paper Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network in Keras. Super-Resolution using Generative Adversarial Networks","title":"Image Super-Resolution using GANs"},{"location":"projects/dl/tensorflow/","text":"Tensorflow \u00b6 Projects done in Tensorflow - either using Graph Execution or Eager Execution. Tensorflow ODE Solver \u00b6 A library built to replicate the TorchDiffEq library built for the Neural Ordinary Differential Equations paper by Chen et al , running entirely on Tensorflow Eager Execution. All credits for the codebase go to @rtqichen for providing an excellent base to reimplement from. Similar to the PyTorch codebase, this library provides ordinary differential equation (ODE) solvers implemented in Tensorflow Eager. For usage of ODE solvers in deep learning applications, see Neural Ordinary Differential Equations paper . As the solvers are implemented in Tensorflow, algorithms in this repository are fully supported to run on the GPU. PySHAC - Sequential Halving and Classification \u00b6 PySHAC is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease.","title":"Tensorflow"},{"location":"projects/dl/tensorflow/#tensorflow","text":"Projects done in Tensorflow - either using Graph Execution or Eager Execution.","title":"Tensorflow"},{"location":"projects/dl/tensorflow/#tensorflow-ode-solver","text":"A library built to replicate the TorchDiffEq library built for the Neural Ordinary Differential Equations paper by Chen et al , running entirely on Tensorflow Eager Execution. All credits for the codebase go to @rtqichen for providing an excellent base to reimplement from. Similar to the PyTorch codebase, this library provides ordinary differential equation (ODE) solvers implemented in Tensorflow Eager. For usage of ODE solvers in deep learning applications, see Neural Ordinary Differential Equations paper . As the solvers are implemented in Tensorflow, algorithms in this repository are fully supported to run on the GPU.","title":"Tensorflow ODE Solver"},{"location":"projects/dl/tensorflow/#pyshac-sequential-halving-and-classification","text":"PySHAC is a python library to use the Sequential Halving and Classification algorithm from the paper Parallel Architecture and Hyperparameter Search via Successive Halving and Classification with ease.","title":"PySHAC - Sequential Halving and Classification"},{"location":"projects/dl/time-series-classification/","text":"Keras Time Series Classifiers / Recurrent Nets \u00b6 Scripts which provide a large number of custom Recurrent Neural Network implementations, which can be dropin replaced for LSTM or GRUs. All of these models are built in Keras or Tensorflow. LSTM Fully Convolutional Networks \u00b6 LSTM FCN models, from the paper LSTM Fully Convolutional Networks for Time Series Classification , augment the fast classification performance of Temporal Convolutional layers with the precise classification of Long Short Term Memory Recurrent Neural Networks. Multivariate LSTM Fully Convolutional Networks \u00b6 MLSTM FCN models, from the paper Multivariate LSTM-FCNs for Time Series Classification , augment the squeeze and excitation block with the state of the art univariate time series model, LSTM-FCN and ALSTM-FCN from the paper LSTM Fully Convolutional Networks for Time Series Classification . Chrono LSTM / Just Another Neural Network (JANET) \u00b6 Keras implementation of the paper The unreasonable effectiveness of the forget gate and the Chrono initializer and Chrono LSTM from the paper Can Recurrent Neural Networks Warp Time? . This model utilizes just 2 gates - forget (f) and context \u00a9 gates out of the 4 gates in a regular LSTM RNN, and uses Chrono Initialization to acheive better performance than regular LSTMs while using fewer parameters and less complicated gating structure. Independent RNN (IndRNN) \u00b6 Keras implementation of the IndRNN model from the paper Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN . Simple Recurrent Unit (SRU) \u00b6 Implementation of Simple Recurrent Unit in Keras. Paper - Training RNNs as Fast as CNNs . This is a naive implementation with some speed gains over the generic LSTM cells, however its speed is not yet 10x that of cuDNN LSTMs Nested LSTMs \u00b6 Keras implementation of Nested LSTMs from the paper Nested LSTMs . Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own inner memory cell. Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM. Multiplicative LSTMs \u00b6 Implementation of the paper Multiplicative LSTM for sequence modelling for Keras 2.0+. Multiplicative LSTMs have been shown to achieve state-of-the-art or close to SotA results for sequence modelling datasets. They also perform better than stacked LSTM models for the Hutter-prize dataset and the raw wikipedia dataset. Minimal RNN \u00b6 Keras implementation of MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks .","title":"Time Series Classification"},{"location":"projects/dl/time-series-classification/#keras-time-series-classifiers-recurrent-nets","text":"Scripts which provide a large number of custom Recurrent Neural Network implementations, which can be dropin replaced for LSTM or GRUs. All of these models are built in Keras or Tensorflow.","title":"Keras Time Series Classifiers / Recurrent Nets"},{"location":"projects/dl/time-series-classification/#lstm-fully-convolutional-networks","text":"LSTM FCN models, from the paper LSTM Fully Convolutional Networks for Time Series Classification , augment the fast classification performance of Temporal Convolutional layers with the precise classification of Long Short Term Memory Recurrent Neural Networks.","title":"LSTM Fully Convolutional Networks"},{"location":"projects/dl/time-series-classification/#multivariate-lstm-fully-convolutional-networks","text":"MLSTM FCN models, from the paper Multivariate LSTM-FCNs for Time Series Classification , augment the squeeze and excitation block with the state of the art univariate time series model, LSTM-FCN and ALSTM-FCN from the paper LSTM Fully Convolutional Networks for Time Series Classification .","title":"Multivariate LSTM Fully Convolutional Networks"},{"location":"projects/dl/time-series-classification/#chrono-lstm-just-another-neural-network-janet","text":"Keras implementation of the paper The unreasonable effectiveness of the forget gate and the Chrono initializer and Chrono LSTM from the paper Can Recurrent Neural Networks Warp Time? . This model utilizes just 2 gates - forget (f) and context \u00a9 gates out of the 4 gates in a regular LSTM RNN, and uses Chrono Initialization to acheive better performance than regular LSTMs while using fewer parameters and less complicated gating structure.","title":"Chrono LSTM / Just Another Neural Network (JANET)"},{"location":"projects/dl/time-series-classification/#independent-rnn-indrnn","text":"Keras implementation of the IndRNN model from the paper Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN .","title":"Independent RNN (IndRNN)"},{"location":"projects/dl/time-series-classification/#simple-recurrent-unit-sru","text":"Implementation of Simple Recurrent Unit in Keras. Paper - Training RNNs as Fast as CNNs . This is a naive implementation with some speed gains over the generic LSTM cells, however its speed is not yet 10x that of cuDNN LSTMs","title":"Simple Recurrent Unit (SRU)"},{"location":"projects/dl/time-series-classification/#nested-lstms","text":"Keras implementation of Nested LSTMs from the paper Nested LSTMs . Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own inner memory cell. Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.","title":"Nested LSTMs"},{"location":"projects/dl/time-series-classification/#multiplicative-lstms","text":"Implementation of the paper Multiplicative LSTM for sequence modelling for Keras 2.0+. Multiplicative LSTMs have been shown to achieve state-of-the-art or close to SotA results for sequence modelling datasets. They also perform better than stacked LSTM models for the Hutter-prize dataset and the raw wikipedia dataset.","title":"Multiplicative LSTMs"},{"location":"projects/dl/time-series-classification/#minimal-rnn","text":"Keras implementation of MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks .","title":"Minimal RNN"},{"location":"research/preprints/","text":"Arxiv Pre-prints \u00b6 Multivariate LSTM-FCNs for Time Series Classification \u00b6 Abstract Over the past decade, multivariate time series classification has been receiving a lot of attention. We propose augmenting the existing univariate time series classification models, LSTM-FCN and ALSTM-FCN with a squeeze and excitation block to further improve performance. Our proposed models outperform most of the state of the art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems. LSTM Fully Convolutional Networks for Time Series Classification \u00b6 Abstract Fully convolutional neural networks (FCN) have been shown to achieve state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared to others. We also explore the usage of attention mechanism to improve time series classification with the Attention Long Short Term Memory Fully Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose fine-tuning as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared to other techniques. Insights into LSTM Fully Convolutional Networks for Time Series Classification \u00b6 Abstract Long Short Term Memory Fully Convolutional Neural Networks (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN) have shown to achieve state-of-the-art performance on the task of classifying time series signals on the old University of California-Riverside (UCR) time series repository. However, there has been no study on why LSTM-FCN and ALSTM-FCN perform well. In this paper, we perform a series of ablation tests (3627 experiments) on LSTM-FCN and ALSTM-FCN to provide a better understanding of the model and each of its sub-module. Results from the ablation tests on ALSTM-FCN and LSTM-FCN show that the these blocks perform better when applied in a conjoined manner. Two z-normalizing techniques, z-normalizing each sample independently and z-normalizing the whole dataset, are compared using a Wilcoxson signed-rank test to show a statistical difference in performance. In addition, we provide an understanding of the impact dimension shuffle has on LSTM-FCN by comparing its performance with LSTM-FCN when no dimension shuffle is applied. Finally, we demonstrate the performance of the LSTM-FCN when the LSTM block is replaced by a GRU, basic RNN, and Dense Block. Adversarial Attacks on Time Series \u00b6 Abstract Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial transformation network (ATN) on a distilled model to attack various time series classification models. The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics the behavior of the attacked classical time series classification models. Our proposed methodology is applied onto 1-Nearest Neighbor Dynamic Time Warping (1-NN ) DTW, a Fully Connected Network and a Fully Convolutional Network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. To the best of our knowledge, such an attack on time series classification models has never been done before. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples and to consider model robustness as an evaluative metric. A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer \u00b6 Abstract Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: neural style transfer with improvements and universal style transfer. We also make a comparison between the different images produced and how they can be qualitatively measured.","title":"Preprints"},{"location":"research/preprints/#arxiv-pre-prints","text":"","title":"Arxiv Pre-prints"},{"location":"research/preprints/#multivariate-lstm-fcns-for-time-series-classification","text":"Abstract Over the past decade, multivariate time series classification has been receiving a lot of attention. We propose augmenting the existing univariate time series classification models, LSTM-FCN and ALSTM-FCN with a squeeze and excitation block to further improve performance. Our proposed models outperform most of the state of the art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.","title":"Multivariate LSTM-FCNs for Time Series Classification"},{"location":"research/preprints/#lstm-fully-convolutional-networks-for-time-series-classification","text":"Abstract Fully convolutional neural networks (FCN) have been shown to achieve state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared to others. We also explore the usage of attention mechanism to improve time series classification with the Attention Long Short Term Memory Fully Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose fine-tuning as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared to other techniques.","title":"LSTM Fully Convolutional Networks for Time Series Classification"},{"location":"research/preprints/#insights-into-lstm-fully-convolutional-networks-for-time-series-classification","text":"Abstract Long Short Term Memory Fully Convolutional Neural Networks (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN) have shown to achieve state-of-the-art performance on the task of classifying time series signals on the old University of California-Riverside (UCR) time series repository. However, there has been no study on why LSTM-FCN and ALSTM-FCN perform well. In this paper, we perform a series of ablation tests (3627 experiments) on LSTM-FCN and ALSTM-FCN to provide a better understanding of the model and each of its sub-module. Results from the ablation tests on ALSTM-FCN and LSTM-FCN show that the these blocks perform better when applied in a conjoined manner. Two z-normalizing techniques, z-normalizing each sample independently and z-normalizing the whole dataset, are compared using a Wilcoxson signed-rank test to show a statistical difference in performance. In addition, we provide an understanding of the impact dimension shuffle has on LSTM-FCN by comparing its performance with LSTM-FCN when no dimension shuffle is applied. Finally, we demonstrate the performance of the LSTM-FCN when the LSTM block is replaced by a GRU, basic RNN, and Dense Block.","title":"Insights into LSTM Fully Convolutional Networks for Time Series Classification"},{"location":"research/preprints/#adversarial-attacks-on-time-series","text":"Abstract Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial transformation network (ATN) on a distilled model to attack various time series classification models. The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics the behavior of the attacked classical time series classification models. Our proposed methodology is applied onto 1-Nearest Neighbor Dynamic Time Warping (1-NN ) DTW, a Fully Connected Network and a Fully Convolutional Network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. To the best of our knowledge, such an attack on time series classification models has never been done before. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples and to consider model robustness as an evaluative metric.","title":"Adversarial Attacks on Time Series"},{"location":"research/preprints/#a-comprehensive-comparison-between-neural-style-transfer-and-universal-style-transfer","text":"Abstract Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: neural style transfer with improvements and universal style transfer. We also make a comparison between the different images produced and how they can be qualitatively measured.","title":"A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer"},{"location":"research/published/","text":"Journal Publications (Deep Learning) \u00b6 LSTM Fully Convolutional Networks for Time Series Classification \u00b6 Abstract Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set. The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques. Microaneurysm detection using fully convolutional neural networks \u00b6 Abstract Backround and Objectives : Diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough. Microaneurysms are the earliest clinical signs of diabetic retinopa- thy. This paper presents an automatic method for detecting microaneurysms in fundus photographies. Methods : A novel patch-based fully convolutional neural network with batch normalization layers and Dice loss function is proposed. Compared to other methods that require up to \ufb01ve processing stages, it requires only three. Furthermore, to the best of the authors\u2019 knowledge, this is the \ufb01rst paper that shows how to successfully transfer knowledge between datasets in the microaneurysm detection domain. Results : The proposed method was evaluated using three publicly available and widely used datasets: E- Ophtha, DIARETDB1, and ROC. It achieved better results than state-of-the-art methods using the FROC metric. The proposed algorithm accomplished highest sensitivities for low false positive rates, which is particularly important for screening purposes. Microaneurysm detection using deep learning and interleaved freezing \u00b6 Abstract Diabetes affects one in eleven adults. Diabetic retinopathy is a microvascular complication of diabetes and the leading cause of blindness in the working-age population. Microaneurysms are the earliest clinical signs of diabetic retinopathy. This paper proposes an automatic method for detecting microaneurysms in fundus photographies. A novel patch-based fully convolutional neural network for detection of microaneurysms is proposed. Compared to other methods that require five processing stages, it requires only two. Furthermore, a novel network fine-tuning scheme called Interleaved Freezing is presented. This procedure significantly reduces the amount of time needed to re-train a network and produces competitive results. The proposed method was evaluated using publicly available and widely used datasets: E-Ophtha and ROC. It outperforms the state-of-the-art methods in terms of free-response receiver operatic characteristic (FROC) metric. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications. Journal Publications (Algorithms Research / Machine Learning) \u00b6 Parallel Quick Sort using Thread Pool Pattern \u00b6 Abstract Sorting algorithms, their implementations and their applications in modern computing necessitates improvements for sorting large data sets quickly and efficiently. This paper will analyze the performance of a multi-threaded quick sort implemented using the thread pool pattern. The analysis will be done by comparing the time required to sort various data sets and their memory constraints, against the native sorting implementations of the Dual Pivot Quicksort and Merge Sort using the Fork-Join framework in the Oracle Java 8 programming language. Analysis is done of the effect of different number of processor (cores) of the test machine, as well as the performance barrier due to the initial time taken to create \u201cp\u201d threads, p being the number of processors. This paper also analyzes the limitations of the inbuilt Java method Arrays.parallelSort() and how the proposed system overcomes this problem. Finally, it also discuss possible improvements to the proposed system to further improve its performance. AdaSort: Adaptive Sorting using Machine Learning \u00b6 Abstract Sorting algorithms and their implementations in modern computing requires improvements in sorting large data sets effectively, both with respect to time and memory consumed. This paper is aimed at reviewing multiple adaptive sorting algorithms, on the basis of selection of an algorithm based on the characteristics of the data set. Machine Learning allows us to construct an adaptive algorithm based on the analysis of the experimental data. A review of algorithms designed using Systems of Algorithmic Algebra and Genetic Algorithms was performed. Both methods are designed to target different use cases. Systems of Algorithmic Algebra is a representation of pseudo code that can be converted to high level code using Integrated toolkit for Design and Synthesis of programs, while the Genetic Algorithm attempts to optimize its fitness function and generate the most successful algorithm.","title":"Published"},{"location":"research/published/#journal-publications-deep-learning","text":"","title":"Journal Publications (Deep Learning)"},{"location":"research/published/#lstm-fully-convolutional-networks-for-time-series-classification","text":"Abstract Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set. The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques.","title":"LSTM Fully Convolutional Networks for Time Series Classification"},{"location":"research/published/#microaneurysm-detection-using-fully-convolutional-neural-networks","text":"Abstract Backround and Objectives : Diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough. Microaneurysms are the earliest clinical signs of diabetic retinopa- thy. This paper presents an automatic method for detecting microaneurysms in fundus photographies. Methods : A novel patch-based fully convolutional neural network with batch normalization layers and Dice loss function is proposed. Compared to other methods that require up to \ufb01ve processing stages, it requires only three. Furthermore, to the best of the authors\u2019 knowledge, this is the \ufb01rst paper that shows how to successfully transfer knowledge between datasets in the microaneurysm detection domain. Results : The proposed method was evaluated using three publicly available and widely used datasets: E- Ophtha, DIARETDB1, and ROC. It achieved better results than state-of-the-art methods using the FROC metric. The proposed algorithm accomplished highest sensitivities for low false positive rates, which is particularly important for screening purposes.","title":"Microaneurysm detection using fully convolutional neural networks"},{"location":"research/published/#microaneurysm-detection-using-deep-learning-and-interleaved-freezing","text":"Abstract Diabetes affects one in eleven adults. Diabetic retinopathy is a microvascular complication of diabetes and the leading cause of blindness in the working-age population. Microaneurysms are the earliest clinical signs of diabetic retinopathy. This paper proposes an automatic method for detecting microaneurysms in fundus photographies. A novel patch-based fully convolutional neural network for detection of microaneurysms is proposed. Compared to other methods that require five processing stages, it requires only two. Furthermore, a novel network fine-tuning scheme called Interleaved Freezing is presented. This procedure significantly reduces the amount of time needed to re-train a network and produces competitive results. The proposed method was evaluated using publicly available and widely used datasets: E-Ophtha and ROC. It outperforms the state-of-the-art methods in terms of free-response receiver operatic characteristic (FROC) metric. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.","title":"Microaneurysm detection using deep learning and interleaved freezing"},{"location":"research/published/#journal-publications-algorithms-research-machine-learning","text":"","title":"Journal Publications (Algorithms Research / Machine Learning)"},{"location":"research/published/#parallel-quick-sort-using-thread-pool-pattern","text":"Abstract Sorting algorithms, their implementations and their applications in modern computing necessitates improvements for sorting large data sets quickly and efficiently. This paper will analyze the performance of a multi-threaded quick sort implemented using the thread pool pattern. The analysis will be done by comparing the time required to sort various data sets and their memory constraints, against the native sorting implementations of the Dual Pivot Quicksort and Merge Sort using the Fork-Join framework in the Oracle Java 8 programming language. Analysis is done of the effect of different number of processor (cores) of the test machine, as well as the performance barrier due to the initial time taken to create \u201cp\u201d threads, p being the number of processors. This paper also analyzes the limitations of the inbuilt Java method Arrays.parallelSort() and how the proposed system overcomes this problem. Finally, it also discuss possible improvements to the proposed system to further improve its performance.","title":"Parallel Quick Sort using Thread Pool Pattern"},{"location":"research/published/#adasort-adaptive-sorting-using-machine-learning","text":"Abstract Sorting algorithms and their implementations in modern computing requires improvements in sorting large data sets effectively, both with respect to time and memory consumed. This paper is aimed at reviewing multiple adaptive sorting algorithms, on the basis of selection of an algorithm based on the characteristics of the data set. Machine Learning allows us to construct an adaptive algorithm based on the analysis of the experimental data. A review of algorithms designed using Systems of Algorithmic Algebra and Genetic Algorithms was performed. Both methods are designed to target different use cases. Systems of Algorithmic Algebra is a representation of pseudo code that can be converted to high level code using Integrated toolkit for Design and Synthesis of programs, while the Genetic Algorithm attempts to optimize its fitness function and generate the most successful algorithm.","title":"AdaSort: Adaptive Sorting using Machine Learning"}]}